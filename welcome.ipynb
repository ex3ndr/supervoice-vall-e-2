{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4246a3a-b1ef-4647-a59b-2ef587f8b487",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (attention.py, line 44)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/gym/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[1], line 2\u001b[0m\n    from supervoice_valle import SupervoceNARModel, Tokenizer\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/data/notebooks/supervoice-vall-e-2/supervoice_valle/__init__.py:4\u001b[0;36m\n\u001b[0;31m    from .attention import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/data/notebooks/supervoice-vall-e-2/supervoice_valle/attention.py:44\u001b[0;36m\u001b[0m\n\u001b[0;31m    attn_bias = torch.zeros(B, , L, L, dtype=q.dtype, device = q.device)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from supervoice_valle import SupervoceNARModel, Tokenizer\n",
    "from train.dataset import load_sampler\n",
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "from vocos import Vocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12c0dc-5f09-45c4-86ea-eb65b87e0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"charactr/vocos-encodec-24khz\").to(\"cuda\")\n",
    "tokenizer = Tokenizer(\"./tokenizer_text.model\")\n",
    "sampler = load_sampler(\"./external_datasets/libriheavy/libriheavy_cuts_small.jsonl.gz\", \"./external_datasets/libriheavy-encodec/\", 1, tokenizer)\n",
    "model = SupervoceNARModel()\n",
    "checkpoint = torch.load(\"./output/valle-35.pt\", map_location = \"cpu\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "# model = model.to(torch.float16)\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "step = checkpoint['step']\n",
    "print(checkpoint['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef3079-0d4b-4930-b808-fb5de6dc615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, text = sampler()\n",
    "print(audio[0], text[0])\n",
    "encoded = audio[0]\n",
    "text = text[0]\n",
    "# id = \"./external_datasets/libriheavy-encodec/9774/extermination_american_bison_1503_librivox_64kb_mp3/exterminationamericanbison_12_hornaday_64kb_99\"\n",
    "# with open(id + \".txt\", 'r') as file:\n",
    "#     text = file.read()\n",
    "#     text = tokenizer.encode(text)\n",
    "# encoded = torch.load(id + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f5b44-907a-430c-a476-c1a120925fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    features = vocos.codes_to_features(tokens.to(\"cuda\"))\n",
    "    bandwidth_id = torch.tensor([2]).to(\"cuda\")  # 6 kbps\n",
    "    return vocos.decode(features, bandwidth_id=bandwidth_id)\n",
    "display(Audio(data=decode(encoded).cpu(), rate=24000))\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffca155-5f1f-4ed8-93e5-3ce803328c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_text = text[0:0].to(\"cuda\")\n",
    "condition_audio = encoded[:,:75*3].to(\"cuda\")\n",
    "audio = encoded[:,75*3:].to(\"cuda\")\n",
    "predicted = [audio[0]]\n",
    "# print(audio[0].to(\"cuda\"))\n",
    "for i in range(1, 8):\n",
    "    p = model(\n",
    "        condition_text = [condition_text], \n",
    "        condition_audio = [condition_audio],\n",
    "        audio = [torch.stack(predicted)],\n",
    "        codec = [i]\n",
    "    )\n",
    "\n",
    "    p = p[0]\n",
    "    p = torch.nn.functional.softmax(p, dim=-1)\n",
    "    # print(p.shape)\n",
    "\n",
    "    # for j in range(len(audio[i])):\n",
    "    #     print(p[j][audio[i][j]].item(), torch.max(p[j]).item())\n",
    "\n",
    "    # Top-3\n",
    "    # p = p[0]\n",
    "    # v, _ = torch.topk(p.unsqueeze(0), 10)\n",
    "    # v = v.squeeze(0)\n",
    "    # p[p < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "    # Sample\n",
    "    # p = torch.nn.functional.softmax(p, dim=-1)\n",
    "    # p = torch.multinomial(p, num_samples=1)\n",
    "    # p = p.squeeze(-1)\n",
    "\n",
    "    p = torch.argmax(p, dim=-1, keepdim=True)\n",
    "    p = p.squeeze(-1)\n",
    "\n",
    "    # print(\"inference\")\n",
    "    # print(p)\n",
    "    # print(audio[i])\n",
    "    predicted.append(p)\n",
    "predicted = torch.stack(predicted)\n",
    "predicted = torch.cat([condition_audio, predicted], dim = 1)\n",
    "display(Audio(data=decode(predicted).cpu(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd71dcb-29aa-4636-b306-6e417f4e5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.positional_embedding_text.weight.T[:,:200].cpu().detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16337a-f2d6-4b81-bffb-cb34258f1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.positional_embedding_text.weight.T[10,0:200].cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918364e1-e649-45a1-8122-00f7df20c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.positional_embedding_audio.weight.T.cpu().detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6b2a3-1ead-498d-97cb-b3f33d588e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.positional_embedding_audio.weight.T[0, 0:2000].cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee5d2d-b9f2-45ed-9b7e-5c388043746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(step - 40000, step, 5000):\n",
    "#     checkpoint = torch.load(\"./output/valle-23.\" + str(i) + \".pt\", map_location = \"cpu\")\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "#     condition_text = text\n",
    "#     condition_audio = encoded[:,:75*3]\n",
    "#     audio = encoded[:,75*3:]\n",
    "#     predicted = [audio[0]]\n",
    "#     for i in range(1, 8):\n",
    "#         p = model(\n",
    "#             condition_text = [condition_text], \n",
    "#             condition_audio = [condition_audio],\n",
    "#             audio = [torch.stack(predicted)],\n",
    "#             codec = [i]\n",
    "#         )\n",
    "#         p = p[0]\n",
    "#         p = torch.nn.functional.softmax(p, dim=-1)\n",
    "#         p = torch.argmax(p, dim=-1, keepdim=True)\n",
    "#         p = p.squeeze(-1)\n",
    "#         predicted.append(p.cpu())\n",
    "#     predicted = torch.stack(predicted)\n",
    "#     predicted = torch.cat([condition_audio, predicted], dim = 1)\n",
    "#     display(Audio(data=decode(predicted).cpu(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b709c08-2592-475e-abaf-9845c12767cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
